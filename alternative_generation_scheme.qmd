---
title: "Current `where`-matrix approach does not preserve relationships, maybe a different procedure can work"
format: html
---

More or less the current approach in mice. I simplified somewhat, by not taking randomness in the regression coefficients into account here, but that should only have a minor influence on the results (and if it affects the results at all, it should be for the better, because the imputations are closer to their actual values).


```{r}
#| label: set-parameters
#| message: false
library(purrr)
library(dplyr)
library(tidyr)
library(magrittr)
library(mice)

set.seed(123)

N <- 1000000
rho <- 0.6
```

## Approximating the `mice` data generation approach

```{r}
#| label: mice-generation-scheme
#| cache: true
#| cache-lazy: false

syn_cors <- map_dbl(1:100, function(x) {
  X1 <- rnorm(N)
  X2 <- rho * X1 + rnorm(N, 0, sqrt(1-rho^2))
  
  fit1 <- lm(X1 ~ X2)
  fit2 <- lm(X2 ~ X1)
  
  X1syn <- predict(fit1) + rnorm(N, 0, sd(fit1$residuals))
  X2syn <- predict(fit2) + rnorm(N, 0, sd(fit2$residuals))
  
  cor(X1syn, X2syn)
})

mean(syn_cors)
```

As can be seen, the correlation is not preserved. In fact, the correlation is dramatically reduced. To see how, consider the following sketch how this works for two variables, $X_1$ and $X_2$ and synthetic variables $\tilde{X_1}$ and $\tilde{X_2}$. The covariance of the generated synthetic variables equals
$$
\begin{align}
& \mathbb{E}[(\tilde{X_1} - \mu_{\tilde{X_1}})(\tilde{X_2} - \mu_\tilde{X_2})] \\
= & \mathbb{E}[\tilde{X_1}\tilde{X_2}] \\
= & \mathbb{E}[(\rho X_2 + \varepsilon_2)(\rho X_1 + \varepsilon_1)] \\
= & \mathbb{E}[\rho^2 X_1 X_2 + \rho X_1 \varepsilon_2 + \rho X_2 \varepsilon_1 + \varepsilon_1\varepsilon_2] \\
= & \mathbb{E}[\rho^2 X_1 X_2] + \mathbb{E}[\rho X_1 \varepsilon_2] + 
\mathbb{E}[\rho X_2 \varepsilon_1] + \mathbb{E}[\varepsilon_1\varepsilon_2] \\
= & \rho^2 (\mathbb{E}[X_1] \mathbb{E}[X_2] + \rho \sigma_{X_1}\sigma_{X_2}) \\
= & \rho^3,
\end{align}
$$
where we make use of the fact that the expectation of the product of two normally distributed random variables equals the product of the expectations of these variables added to correlation of these two variables multiplied with the product of the square root of the variances of these two variables (which equals $\rho^3 = 0.6^3 = `r 0.6^3`$ in the current simulation). Since the residuals of both models are uncorrelated with each other, but also with the values of the other variable, these terms drop of. 

## Alternative iteration scheme for missing data


An alternative generation scheme can be considered as below, which is, I think, closer to what should happen. 

```{r}
#| label: alternative-generation-scheme
#| cache: true

N <- 10000
P <- 3
S <- matrix(rho, P, P)
diag(S) <- 1


syns <- map(1:100, function(x) {

  X <- rnorm(N*P) |> matrix(N, P) %*% chol(S)
  syn <- X
  
  for (i in 1:ncol(X)) {
    fit <- lm(X[,i] ~ X[,-i])
    b <- fit$coefficients
    nb <- length(b)
    b_se <- vcov(fit)
    pred <- cbind(1, syn[,-i]) %*% (b + c(rnorm(b) |> matrix(1, nb) %*% chol(b_se)))
    syn[,i] <- pred + rnorm(N, 0, sd(fit$residuals))
  }
  syn
})

syns |>
  map_dfr(~data.frame(.x) |>
            summarize(across(.fns = list(mean = mean, var = var))) |>
            pivot_longer(cols = everything(),
                         names_sep = "_",
                         names_to = c("var", "stat"))) |>
  group_by(var, stat) |>
  summarize(value = mean(value))

S

syns |>
  map(var) |>
  {\(x) reduce(x, `+`) / 100}()

```

### Iterating with the new imputation scheme

Now, let's test whether the new iteration scheme doesn't converge to outer space, when we use more than a single iteration. This would give some confidence that we can use a similar procedure to solve for missing data and generate synthetic data simultaneously.

```{r}
#| label: test-iter
#| cache: true

syns_iter <- map(1:100, function(x) {

  X <- rnorm(N*P) |> matrix(N, P) %*% chol(S)
  syn <- X
  
  for (t in 1:100) {
    for (i in 1:ncol(X)) {
      fit <- lm(X[,i] ~ X[,-i])
      b <- fit$coefficients
      nb <- length(b)
      b_se <- vcov(fit)
      pred <- cbind(1, syn[,-i]) %*% (b + c(rnorm(nb) |> matrix(1, nb) %*% chol(b_se)))
      syn[,i] <- pred + rnorm(N, 0, sd(fit$residuals))
    }
  }
  syn
})

syns_iter |>
  map_dfr(~data.frame(.x) |>
            summarize(across(.fns = list(mean = mean, var = var))) |>
            pivot_longer(cols = everything(),
                         names_sep = "_",
                         names_to = c("var", "stat"))) |>
  group_by(var, stat) |>
  summarize(value = mean(value) |> round(4))

S

syns_iter |>
  map(var) |>
  {\(x) reduce(x, `+`) / 100}()

```

### Coverages with this new iteration scheme

Let's additionally test whether we can get nominal coverage rate with the current approach (which is a minimum for being applicable in practice).

```{r}
#| label: test-coverages-new-approach
#| cache: true

beta <- c(0, solve(t(S[2:3,2:3]) %*% S[2:3,2:3]) %*% S[2:3,2:3] %*% S[1,2:3])

coverage <- function(est, true) {
  est |>
    mutate(True_Est = rep(true, nrow(est)/length(true))) |>
    group_by(Var) |>
    summarize(True_Est = mean(True_Est),
              Imp_Est = mean(Est),
              SE = mean(SE),
              Cov = mean(Low < True_Est & True_Est < Upp))
}

inferences <- function(fitlist, rule) {
  fit <- pool(fitlist, rule = rule) %$% pooled
  data.frame(Var = fit[,1],
             Est = fit[,3],
             SE  = sqrt(fit[,6]),
             Low = fit[,3] - qt(0.975, fit[,8]) * sqrt(fit[,6]),
             Upp = fit[,3] + qt(0.975, fit[,8]) * sqrt(fit[,6]))
}

future::plan(future::multisession())

syns_cov <- furrr::future_map(1:500, function(x) {

  X <- rnorm(N*P) |> matrix(N, P) %*% chol(S)
  syn <- rep(list(X), 5)
  
  map(syn, function(s) {
    for (i in 1:ncol(X)) {
      fit <- lm(X[,i] ~ X[,-i])
      b <- fit$coefficients
      nb <- length(b)
      b_se <- vcov(fit)
      pred <- cbind(1, s[,-i]) %*% (b + c(rnorm(nb) |> matrix(1, nb) %*% chol(b_se)))
      s[,i] <- pred + rnorm(N, 0, sd(fit$residuals))
    }
    s |> data.frame()
  })
}, .options = furrr::furrr_options(seed = TRUE))

syns_cov |>
  map_dfr(~ map(.x, function(x) lm(X1 ~ X2 + X3, x)) |>
            inferences(rule = "reiter2003")) |>
  coverage(beta)
```

## Combine imputation with synthetic data generation

Test variance estimator without adjusted degrees of freedom.

```{r}
#| label: test-imp-synthetic
#| cache: true
syns_miss <- furrr::future_map(1:500, function(x) {

  X <- rnorm(N*P) |> matrix(N, P) %*% chol(S) |> data.frame()
  amp <- ampute(X, mech = "MCAR")$amp
  imp <- mice(amp, method = "norm", printFlag = F) |> complete("all")
  
  map(imp, function(i) {
    s <- X <- as.matrix(i)
    
    for (i in 1:ncol(X)) {
      fit <- lm(X[,i] ~ X[,-i])
      b <- fit$coefficients
      nb <- length(b)
      b_se <- vcov(fit)
      pred <- cbind(1, s[,-i]) %*% (b + c(rnorm(nb) |> matrix(1, nb) %*% chol(b_se)))
      s[,i] <- pred + rnorm(N, 0, sd(fit$residuals))
    }
    s |> data.frame()
  })
}, .options = furrr::furrr_options(seed = TRUE))

syns_cov |>
  map_dfr(function(s) {
    fits <- map(s, function(x) lm(X1 ~ X2 + X3, x))
    pool(fits) %$% pooled
  }) |> 
  mutate(SE = sqrt(b + b/m),
         df = dfcom,
         Low = estimate - qt(0.975, df) * SE,
         Upp = estimate + qt(0.975, df) * SE) |>
  select(Var = term,
         Est = estimate,
         SE = SE, 
         Low = Low,
         Upp = Upp) |>
  coverage(beta)
```

Unfortunately doesn't work.