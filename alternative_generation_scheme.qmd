---
title: "Current `where`-matrix approach does not preserve relationships, maybe a different procedure can work"
format: html
---

More or less the current approach in mice. I simplified somewhat, by not taking randomness in the regression coefficients into account here, but that should only have a minor influence on the results (and if it affects the results at all, it should be for the better, because the imputations are closer to their actual values).


```{r}
#| message: false
library(purrr)
library(dplyr)
library(tidyr)

set.seed(123)

N <- 1000000
rho <- 0.6

syn_cors <- map_dbl(1:100, function(x) {
  X1 <- rnorm(N)
  X2 <- rho * X1 + rnorm(N, 0, sqrt(1-rho^2))
  
  fit1 <- lm(X1 ~ X2)
  fit2 <- lm(X2 ~ X1)
  
  X1syn <- predict(fit1) + rnorm(N, 0, sd(fit1$residuals))
  X2syn <- predict(fit2) + rnorm(N, 0, sd(fit2$residuals))
  
  cor(X1syn, X2syn)
})

mean(syn_cors)
```

As can be seen, the correlation is not preserved. In fact, the correlation is dramatically reduced. To see how, consider the following sketch how this works for two variables, $X_1$ and $X_2$ and synthetic variables $\tilde{X_1}$ and $\tilde{X_2}$. The covariance of the generated synthetic variables equals
$$
\begin{align}
& \mathbb{E}[(\tilde{X_1} - \mu_{\tilde{X_1}})(\tilde{X_2} - \mu_\tilde{X_2})] \\
= & \mathbb{E}[\tilde{X_1}\tilde{X_2}] \\
= & \mathbb{E}[(\rho X_2 + \varepsilon_2)(\rho X_1 + \varepsilon_1)] \\
= & \mathbb{E}[\rho^2 X_1 X_2 + \rho X_1 \varepsilon_2 + \rho X_2 \varepsilon_1 + \varepsilon_1\varepsilon_2] \\
= & \mathbb{E}[\rho^2 X_1 X_2] + \mathbb{E}[\rho X_1 \varepsilon_2] + 
\mathbb{E}[\rho X_2 \varepsilon_1] + \mathbb{E}[\varepsilon_1\varepsilon_2] \\
= & \rho^2 (\mathbb{E}[X_1] \mathbb{E}[X_2] + \rho \sigma_{X_1}\sigma_{X_2}) \\
= & \rho^3,
\end{align}
$$
where we make use of the fact that the expectation of the product of two normally distributed random variables equals the product of the expectations of these variables added to the correlation of these two variables multiplied with the product of the square root of the variances of these two variables (which equals $\rho^3 = 0.6^3 = `r 0.6^3`$ in the current simulation). Since the residuals of both models are uncorrelated with each other, but also with the values of the other variable, these terms drop of. 


An alternative generation scheme can be considered as below, which is, I think, closer to what should happen. 

```{r}
N <- 10000
P <- 3
S <- matrix(rho, P, P)
diag(S) <- 1


syns <- map(1:100, function(x) {

  X <- rnorm(N*P) |> matrix(N, P) %*% chol(S)
  syn <- X
  
  for (i in 1:ncol(X)) {
    fit <- lm(X[,i] ~ X[,-i])
    b <- fit$coefficients
    nb <- length(b)
    b_se <- vcov(fit)
    pred <- cbind(1, syn[,-i]) %*% (b + c(rnorm(b) |> matrix(1, nb) %*% chol(b_se)))
    syn[,i] <- pred + rnorm(N, 0, sd(fit$residuals))
  }
  syn
})

syns |>
  map_dfr(~data.frame(.x) |>
            summarize(across(.fns = list(mean = mean, var = var))) |>
            pivot_longer(cols = everything(),
                         names_sep = "_",
                         names_to = c("var", "stat"))) |>
  group_by(var, stat) |>
  summarize(value = mean(value))

S

syns |>
  map(var) |>
  {\(x) reduce(x, `+`) / 100}()

```
